{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LSTMS and RNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a defect of RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are unable to keep data over many iterations because they run into the problem of vanishing gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the inputs and outputs to an LSTM cell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LSTM cell has the following inputs:\n",
    "1. Hidden State\n",
    "2. Cell State\n",
    "3. Actual Input\n",
    "\n",
    "An LSTM cell has the following outputs: \n",
    "1. Cell State\n",
    "2. Hidden State\n",
    "3. Actual output\n",
    "\n",
    "The hidden state and the actual output of an LSTM cell are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What corresponds to long term memory and what corresponds to short term memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long term memory corresponds to the cell state whilc short term memory corresponds to the hidden state. This hidden state is also the output of the LSTM cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the various gates in an LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LSTM has the following gate\n",
    "1. Forget\n",
    "2. Input \n",
    "3. Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do each of the gates do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forget Gate**\n",
    "\n",
    "The forget gate is used to modify the cell state, which holds the long term memory. The LTM is modified in such a way that it loses some information.\n",
    "\n",
    "**Input Gate**\n",
    "\n",
    "This input gate is used to update the LTM based on the input and the hidden state.\n",
    "\n",
    "**Update Gate**\n",
    "\n",
    "This final gate is used to modify the hidden state, for the next timestep. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the forget gate work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forget gate requires both the input at the current timestep and the hidden state from the previous time step. \n",
    "\n",
    "Input : $x_t$\n",
    "\n",
    "Hidden State : $h_{t-1}$\n",
    "\n",
    "$f_t = \\sigma({W_f[x_t, h_{t-1}]} + b_f)$ \n",
    "\n",
    "This can be considered to be a forget factor, which is multiplied with the previous cell state $C_{t-1}$\n",
    "\n",
    "$\\therefore$ the Forget Gate works as: $C_{t-1} * f_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the Input Gate work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input gate requires two input values:\n",
    "\n",
    "1. Hidden State from previous timestep : $h_{t-1}$\n",
    "\n",
    "2. Input at current timestep : $x_t$\n",
    "\n",
    "This is a bit of a complicated gate which works as follows:\n",
    "\n",
    "We initially calculate two intermediate values, \n",
    "\n",
    "$i_t = \\sigma(W_i\\cdot[x_t, h_{t-1}] + b_i)$\n",
    "\n",
    "$\\tilde{C_t} = \\tanh({W_c \\cdot [x_t, h_{t-1}] + b_c})$\n",
    "\n",
    "It is also in this gate, that the cell state is modified. \n",
    "\n",
    "$C_t = i_t * C_{t-1} + f_t * \\tilde{C_t}$\n",
    "\n",
    "This is the modified cell state which is propogated to the next timestep.\n",
    "\n",
    "When $i_t$ is multiplied with $C_{t-1}$, it is element wise multiplication. Likewise, when $f_t$ is multiplied with $\\tilde{C_t}$, it is also elementwise multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the update gate work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the update gate is to modify the hidden state that is propogated to the next timestep. It requires two external inputs and an internal input. \n",
    "\n",
    "**External Inputs :** \n",
    "\n",
    "1. Hidden State from previous timestep : $h_{t-1}$\n",
    "\n",
    "2. Input from current timestep : $x_t$\n",
    "\n",
    "**Internal Input :**\n",
    "\n",
    "1. New Cell State : $C_t$\n",
    "\n",
    "We calculate an intermediate output called $o_t$ which is multiplied with the $\\tanh$ of the cell state of the current timestep, $C_t$\n",
    "\n",
    "$o_t = \\sigma ( W_o \\cdot [x_t, h_{t-1}] + b_o)$\n",
    "\n",
    "$\\therefore h_t = o_t * \\tanh{C_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What intuition can you obtain from the working of the gates in an LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever a sigmoid, $\\sigma()$, activation function is used, it corresponds to losing some information. \n",
    "\n",
    "Likewise, whenever a tanh, $\\tanh()$, activation is used, it corresponds to the cell learning something. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Understanding LSTMS](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
